{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IjLiVOEOwYb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login #login to hugginface\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl3AZTlz2B-F",
        "outputId": "7ea4b72a-b18e-4ed4-d70a-6db73ec2538b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `final` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `final`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.cache/huggingface/token  #check if your token is stored\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycsrrzzZ6D_8",
        "outputId": "4b2d531e-2580-4e09-8b78-d1c265ec5129"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /root/.cache/huggingface/token: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami\n",
        "print(whoami())"
      ],
      "metadata": {
        "id": "C8BC1xL96SWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "def tunnel_prep():\n",
        "    for f in ('cloudflared-linux-amd64', 'logs.txt', 'nohup.out'):\n",
        "        try:\n",
        "            os.remove(f'/content/{f}')\n",
        "            print(f\"Deleted {f}\")\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -q\n",
        "    !chmod +x cloudflared-linux-amd64\n",
        "    !nohup /content/cloudflared-linux-amd64 tunnel --url http://localhost:8501 &\n",
        "    url = \"\"\n",
        "    while not url:\n",
        "        time.sleep(1)\n",
        "        result = !grep -o 'https://.*\\.trycloudflare.com' nohup.out | head -n 1\n",
        "        if result:\n",
        "            url = result[0]\n",
        "    return display(HTML(f'Your tunnel URL <a href=\"{url}\" target=\"_blank\">{url}</a>'))"
      ],
      "metadata": {
        "id": "PGWEbFZrEvsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq -U langchain-huggingface\n",
        "!pip install -qqq -U langchain\n",
        "!pip install -qqq -U langchain-community\n",
        "!pip install -qqq -U faiss-cpu\n",
        "!pip install -qqq -U streamlit\n"
      ],
      "metadata": {
        "id": "mnIx0J-iFdOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e25a9e2-4644-4eba-c0b8-f3f553a0b03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_app.py\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "import streamlit as st\n",
        "import base64\n",
        "from PIL import Image\n",
        "\n",
        "#HuggingFace LLM-Endpoint\n",
        "hf_token = os.getenv(\"HF_TOKEN\")  #load Toekn from your secrets\n",
        "hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=hf_model,\n",
        "    temperature=0.5,\n",
        "    top_p=0.9,\n",
        "    huggingface_api_key=hf_token,\n",
        ")\n",
        "\n",
        "# Your Embeddings-Modell\n",
        "embedding_model = \"sentence-transformers/msmarco-MiniLM-L-6-v3\"\n",
        "embeddings_folder = \"/content/\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model, cache_folder=embeddings_folder)\n",
        "\n",
        "#Load FAISS-Index\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "#Prompt Template\n",
        "input_template = \"\"\"Answer the question based only on the following context. Keep your answers short and succinct.\n",
        "\n",
        "Context to answer question:\n",
        "{context}\n",
        "\n",
        "Question to be answered: {question}\n",
        "Response:\"\"\"\n",
        "prompt = PromptTemplate(template=input_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "#Build RAG-Chain with  RetrievalQA\n",
        "@st.cache_resource\n",
        "def init_rag_bot():\n",
        "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "rag_bot = init_rag_bot()\n",
        "\n",
        "##### Streamlit App #####\n",
        "\n",
        "st.title(\"THE AI ASSISTANT\")\n",
        "\n",
        "#Initialise Chat-Historie\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "#sho chat history\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"], avatar=message[\"avatar\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "# Define avatars for human and assistant\n",
        "user_avatar = \"🔎\"  #Human avatar (can be an emoji, image URL, etc.)\n",
        "assistant_avatar = \"🤖\"  #Assistant avatar (can be an emoji, image URL, etc.)\n",
        "\n",
        "#add the human message with avatar only once (when the user submits)\n",
        "if user_input := st.chat_input(\"Ask me anything!\"):\n",
        "    #display human message and append to the history\n",
        "    st.chat_message(\"human\", avatar=user_avatar).markdown(user_input)\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": user_input, \"avatar\": user_avatar})\n",
        "\n",
        "    with st.spinner(\"Please wait while I think...\"):\n",
        "        answer = rag_bot({\"query\": user_input})\n",
        "        response = answer.get(\"result\", \"I couldn't find an answer.\")\n",
        "        sources = []\n",
        "        if \"source_documents\" in answer:\n",
        "            for doc in answer[\"source_documents\"]:\n",
        "                source_name = doc.metadata.get(\"source\", \"Unknown Source\")\n",
        "                sources.append(source_name)\n",
        "        if sources:\n",
        "            source_text = \"\\n\".join(f\"- {source}\" for source in sources)\n",
        "        else:\n",
        "            source_text = \"No sources found.\"\n",
        "\n",
        "        response_with_sources = f\"{response}\\n\\n**Sources:**\\n{source_text}\"\n",
        "\n",
        "        st.chat_message(\"assistant\", avatar=assistant_avatar).markdown(response_with_sources)\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_with_sources, \"avatar\": assistant_avatar})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaEUMmYNS34w",
        "outputId": "c1fa0cfd-72a1-46de-bf6e-fbc2855de59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing rag_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tunnel_prep()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yvc9sHcRE3ux",
        "outputId": "69826242-aeb6-4553-da53-1d9021a07733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Your tunnel URL <a href=\"https://finals-lady-mailto-editorials.trycloudflare.com\" target=\"_blank\">https://finals-lady-mailto-editorials.trycloudflare.com</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run rag_app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "CofRCyae9O0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging"
      ],
      "metadata": {
        "id": "072r5g5F1723"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIONAL: Debugging (remove or set DEBUG = False for production)\n",
        "DEBUG = False\n",
        "if DEBUG:\n",
        "    st.write(\"Debugging Chat History:\", st.session_state.messages)"
      ],
      "metadata": {
        "id": "Jhz_K-JImjwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#teste con\n",
        "import requests\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer <your token>\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"inputs\": \"Can you provide a summary of the key benefits of AI in cybersecurity?\"\n",
        "}\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "response = requests.post(f\"https://api-inference.huggingface.co/models/{model_name}\", headers=headers, json=data)\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1XKcOTw3GFN",
        "outputId": "d54dae24-8f08-4f07-81f0-61f89617de7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Can you provide a summary of the key benefits of AI in cybersecurity?\\n\\nAI in cybersecurity offers several significant benefits:\\n\\n1. Enhanced threat detection: AI can analyze large amounts of data at high speeds, allowing for the rapid detection of patterns and anomalies that might indicate a cyber threat. This proactive approach helps to stop threats before they cause substantial damage.\\n\\n2. Improved accuracy: AI models learn from past attacks and successive data, becoming more accurate over time at identifying threats and correctly distinguishing them from false positives. This reduces the burden on human security analysts.\\n\\n3. Reduced human error: Human error is a common factor in security breaches. AI can mitigate these risks by automating routine tasks, minimizing the chances of errors, and ensuring thorough and thorough security measures.\\n\\n4. Real-time response: AI can analyze threats in real-time, enabling security teams to respond swiftly to attacks, reducing the window of opportunity for cybercriminals.\\n\\n5. Scalability: AI solutions can easily scale to handle growing data volumes as well as evolving threat landscapes, making them flexible enough to adapt to an organization's changing security needs.\\n\\n6. Cost-effectiveness: By automating tasks and complementing human analysts, AI reduces the need for extensive staffing, resulting in cost savings for organizations.\\n\\n7. Threat prediction and mitigation: AI can forecast potential threats based on past occurrences and current trends, empowering security teams to take preventative measures before threats materialize.\\n\\n8. Continuous learning: AI's ability to learn from past experiences and adapt to new threats allows it to evolve alongside cybercriminals, continually improving its protective capabilities. This results in a more resilient and robust security posture for organizations.\"}]\n"
          ]
        }
      ]
    }
  ]
}